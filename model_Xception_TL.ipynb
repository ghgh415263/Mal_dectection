{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_Xception_TL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWMCiVL6sg77gmiGEu14++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghgh415263/Mal_dectection/blob/main/model_Xception_TL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/data/malimg_dataset.zip\" -d \"/content/dataset\""
      ],
      "metadata": {
        "id": "iIJB-0q6FCnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXiV41SdDKt3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "\n",
        "import pathlib\n",
        "\n",
        "class CustomCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "        current_decayed_lr = self.model.optimizer._decayed_lr(tf.float32).numpy()\n",
        "        print(\"current decayed lr: {:0.7f}\".format(current_decayed_lr))\n",
        "\n",
        "DATA_PATH = \"/content/dataset\"\n",
        "batch_size = 32\n",
        "img_height = 299\n",
        "img_width = 299\n",
        "val_split = 0.2\n",
        "interpolation = 'bicubic' #bicubic bilinear\n",
        "seed = 123\n",
        "\n",
        "data_dir = pathlib.Path(DATA_PATH)\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=val_split,\n",
        "  color_mode='rgb',\n",
        "  interpolation=interpolation,\n",
        "  subset=\"training\",\n",
        "  seed=seed,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=val_split,\n",
        "  color_mode='rgb',\n",
        "  interpolation=interpolation,\n",
        "  subset=\"validation\",\n",
        "  seed=seed,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "base_model = Xception(weights=\"imagenet\", include_top=False, input_shape=(img_height,img_width,3)) #pooling=\"avg\"\n",
        "base_model.trainable=False\n",
        "\n",
        "model = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    #keras.layers.Dropout(0.5),\n",
        "    layers.Dense(25, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=2340,\n",
        "    decay_rate=0.2,\n",
        "    staircase=False) #staircase=False\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/Colab_Notebooks/save/training-vgg16-1layer-bicubic2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "#latest = tf.train.latest_checkpoint(\"/content/drive/MyDrive/Colab_Notebooks/malware_model192x192/save/training-res50-1layer\")\n",
        "#model.load_weights(latest)\n",
        "\n",
        "#round(model.optimizer.lr.numpy(), 5)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch > 25:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.15)\n",
        "\n",
        "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    period=200)\n",
        "\n",
        "history = model.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=[cp_callback, CustomCallback(), scheduler_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_test = np.array([])\n",
        "y_pred = np.array([])\n",
        "\n",
        "for x,y in val_ds:\n",
        "  y_test = np.append(y_test, y.numpy(), axis=0)\n",
        "  tmp = (model.predict(x)).argmax(axis=-1)\n",
        "  y_pred = np.append(y_pred, tmp, axis=0)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_ds.class_names)\n",
        "fig, ax = plt.subplots(figsize=(16,16))\n",
        "\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cHXrerX3E_Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(epochs, loss, 'ro-', label='Training loss', markersize = 4)\n",
        "plt.plot(epochs, val_loss, 'bo-', label='Validation loss', markersize = 4)\n",
        "plt.title('Training and validation loss', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.legend(fontsize=14)\n",
        "plt.xticks(range(len(acc) + 1), range(len(acc) + 1))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fBmi7VC0FHRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()   # 그림을 초기화합니다\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(epochs, acc, 'ro-', label='Training acc', markersize = 4)\n",
        "plt.plot(epochs, val_acc, 'bo-', label='Validation acc', markersize = 4)\n",
        "plt.title('Training and validation accuracy', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.legend(fontsize=14)\n",
        "plt.xticks(range(len(acc) + 1), range(len(acc) + 1))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jRSWEq4TFID4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%-8s\" % \"epoch\", \"%-23s\" % \"train_loss\", \"%-23s\" % \"val_loss\", \"%-23s\" % \"train_acc\", \"%-23s\" % \"val_acc\")\n",
        "print(\"=\" * 99)\n",
        "for i in range(1,len(acc)+1):\n",
        "        print(\"%-8s\" %i, \"%-23s\" % loss[i-1], \"%-23s\" % val_loss[i-1], \"%-23s\" % acc[i-1], \"%-23s\" % val_acc[i-1])"
      ],
      "metadata": {
        "id": "7yGGDua3FL_Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}